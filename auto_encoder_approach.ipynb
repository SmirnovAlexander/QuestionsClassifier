{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "auto_encoder_approach.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMC-aqVxXd5b"
      },
      "source": [
        "This notebook contains neural network approach. \n",
        "It works like simple AutoEncoder: encodes TF-IDF vectors with `char_wb` analyzer. After encoding result codes are fitted into simple `LogisticRegression` model with balanced weights. We filter a lot of non-expert sentences to make training process more stable and dataset less imbalanced.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU_eME4Z3UoI"
      },
      "source": [
        "#TODO\n",
        "*  Try to use TripletLoss with hard samples mining\n",
        "*  Try to use TripletLoss with multiple positive and negative samples and one anchor for more stable training\n",
        "*  Experiment with architecture and losses (maybe -MSE(anchore, negative) will work better, because it is possible that we don't need clusters) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pM7YfPw_UFhS",
        "outputId": "002051a7-3469-4c0d-d965-21c06c45ddf4"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import re, nltk\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoaQunHwUFha"
      },
      "source": [
        "# Preprocessing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C18lSUkre3kl"
      },
      "source": [
        "What was done for preprocessing?\n",
        "- found patterns for expert questions\n",
        "- tried to use deeppavlov pretrained model for NER, but understood that this method is too time-consuming\n",
        "- tried to use spellchecker, but there were corrected too many word including good ones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ua_w6PEzz_A"
      },
      "source": [
        "path = '/content/data.csv'\n",
        "data = pd.read_csv(path, sep='\\t',  delimiter=';') \n",
        "\n",
        "data = data.astype({\"Question\": \"string\"})\n",
        "\n",
        "path_l = '/content/train.csv'\n",
        "y = pd.read_csv(path_l,  delimiter=';')\n",
        "\n",
        "train = pd.merge(data, y, on='ID')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPX8E2cpeE7C"
      },
      "source": [
        "Find most popular non-experts words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAwnbYG6d8F_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a07473a-defe-442d-9411-40f94e9ed874"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "X = np.array(train[\"Question\"].values).reshape(-1, 1)\n",
        "X, y1 = ros.fit_resample(X, train[\"Answer\"].values.reshape(-1, 1)) \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y1, test_size=0.33, random_state=42)\n",
        "X_train, X_test, y_train, y_test = X_train[:,0], X_test[:,0], y_train, y_test\n",
        "\n",
        "\n",
        "N = 6\n",
        "pipeline = Pipeline([\n",
        "    ('features', TfidfVectorizer(ngram_range=(1, N), max_features=10000)),\n",
        "    ('clf', LogisticRegression())\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "features = {v: k for k, v in pipeline.named_steps['features'].vocabulary_.items()}\n",
        "k = 300\n",
        "candidates = {}\n",
        "candidates[0] = [features[i] for i in np.argsort(pipeline.named_steps['clf'].coef_[0])[:k]]\n",
        "candidates[1] = [features[i] for i in np.argsort(pipeline.named_steps['clf'].coef_[0])[-k:]][::-1]\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh0pl43SgDR9",
        "outputId": "d5d8eaca-64a7-4bf6-eebd-3449f9b4ec1b"
      },
      "source": [
        "print(candidates[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['это', 'назовите', 'будет', 'почему', 'сколько', 'он', 'клевер', 'клевера', 'формула', 'скольки', 'самая', 'куда', 'жизнь', 'кем', 'находиться', 'номер', 'русский', 'дата', 'пушкин', 'не', 'столица', 'зачем', 'самый', 'можно', 'сталин', 'то', 'вконтакте', 'так', 'всех', 'что', 'числа', 'чего', 'мы', 'имя', 'сейчас', 'нужно', 'автомобиля', 'нашей', 'продолжите', 'месте', 'по', 'на земле', 'на', 'какую', 'год', 'или', 'кого', 'они', 'кто', 'когда', 'брат', 'петр', 'мая', 'когда родился', 'появилось', 'ниже', 'родина', 'праздник', 'со', 'как', 'про', 'песни', 'ленин', 'свою', 'дом', 'их', 'тока', 'где', 'который', 'лучший', 'фамилия', 'популярная', 'нет', 'петербург', 'ног', 'может', 'главный', 'остров', 'картину', 'часто', 'вратарь', 'кто первым', 'белый', 'страна', 'мультфильм', 'самое', 'аниме', 'мультике', 'если', 'одном', 'бы', 'об', 'xx', 'выйграл', 'высокая гора', 'какого', 'снимался', 'певица', 'богов', 'принято', 'крещение руси', 'какой самый большой', 'именно', 'вопрос', 'телефон', 'нельзя', 'своим', 'цветов', 'город владивосток', 'льва', 'императора', 'говорят', 'клевере', 'есть', 'бывает', 'санкт', 'которое', 'мне', 'песня', 'себе', '13', 'австралии', 'мире', 'за что', 'павел', 'военных', 'все', 'стикеры', 'использовали', 'угла', 'фраза', 'санкт петербург', 'альбом', 'килограмм', 'году было', 'птицы', 'третий', 'первый', 'лап', 'только', '28', 'начало', 'жизней', 'звучит', 'на первом', 'выберите', 'путин', 'чм', 'of', 'n2', 'природе', 'n1', 'составляет', 'словом', 'придумали', 'колец', 'каком году было', '12', 'нету', 'лучше', 'собака', 'минут', 'как раньше', 'буквы', 'этой', 'этом', 'очень', 'пух', 'наиболее', 'башни', 'газ', 'расположены', 'ленина', 'школе', 'день', 'благодаря', 'будут', 'n3', 'президентом ссср', 'картина', 'художника', 'живёт', 'растение', '2018', 'гол', 'воды', 'сколько золотых', 'вид', '10', 'свой', 'сериал', 'названия', 'число', 'всем', 'меньше', 'чтобы', 'гора', 'команд', 'этого', 'денег', 'первая', 'какой планете', 'стадион', 'отличается', 'когда было', 'сборной', 'выражение', 'мультика', 'самая высокая гора', 'клеток', 'песню', 'россии', 'фильмах', 'картине', 'голову', 'длина', 'зодиака', 'из чего делают', 'чего делают', 'без', 'чай', 'роль', 'получить', 'помощью', 'путина', 'самые', 'чемпионов', 'чисел', 'том что', 'какое', 'президент', 'групп', 'чем', 'является символом', 'становился чемпионом', 'солнцу', 'кто первый', 'едят', 'счету', 'звезда', 'ходит', 'реал', 'прожил', 'каком слове', 'городов россии', 'сколько планет солнечной', 'из трёх', 'планеты', 'начинается', 'этот', 'владивосток', 'последний', 'официально', 'российский', 'историю', 'правильно', 'россией', 'вопросов', 'момент', 'жизни', 'того', 'животных не', 'должна', 'знак', 'же', 'фрукт', 'вк', 'теле', 'сколько планет солнечной системе', 'коком году', 'коком', 'онлайн', 'придумал', 'из', 'сезоне', 'эти', 'одноименном', 'чемпион', 'под', 'каждый', 'глубина', 'равно', 'конце', 'ему', 'какой', 'эпохи', 'котов', 'войне', 'алфавита', 'выйграла', 'слово', 'самая первая', 'сколько длилась', 'выпущен', 'сегодня', 'вышло', 'был выпущен', 'что на', 'сергей', 'сделать']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl5glANUz1pm"
      },
      "source": [
        "def get_preprocess_data(X):\n",
        "    \"\"\"\n",
        "    We found some patterns usual for expert questions:\n",
        "    - Question answer at the end\n",
        "    - Capital letter at the start\n",
        "    - 'это' only with long dash before\n",
        "    - No special symbols like '...%!+'\n",
        "    - More than 6 words in question\n",
        "    - Right using of comma like 'word, word'\n",
        "    - No frequent non-expert words such as \\['почему', 'будет', 'клевер', 'назовите', 'клевера'\\] \n",
        "\n",
        "    Args: \n",
        "        X - dataframe with columns [id, questions]\n",
        "    \"\"\"\n",
        "\n",
        "    no_expert_words = ['почему', 'будет', 'клевер', 'назовите', 'клевера', 'нельзя', 'нет', 'то', 'метро']\n",
        "    regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")\n",
        "    \n",
        "    def normalize_text(text):\n",
        "        # lowercase text\n",
        "        text = str(text).lower()\n",
        "        # remove non-UTF\n",
        "        text = text.encode(\"utf-8\", \"ignore\").decode()\n",
        "        # remove punktuation symbols\n",
        "        text = \" \".join(regex_tokenizer.tokenize(text))\n",
        "        return text\n",
        "    \n",
        "    questions = X.iloc[:, 1]\n",
        "    vals = questions.values\n",
        "    \n",
        "    end_question = []\n",
        "    fst_capital = []\n",
        "    no_expert_eto = []\n",
        "    special_signs = []\n",
        "    good_special_words= []\n",
        "    not_enough_words = []\n",
        "    no_expert = []\n",
        "    bad_comma = []\n",
        "\n",
        "    for i in tqdm(range(len(vals))):\n",
        "        x = vals[i]\n",
        "        end_question.append(int(bool(re.search(r'.*?\\?$', x))))\n",
        "        fst_capital.append(int(bool(re.search(r'^[А-Я]', x))))\n",
        "        no_expert_eto.append(int(bool(re.search(r'(\\- это|[^\\–] это)[^а-яА-Я]', x))))\n",
        "        special_signs.append(int(bool(re.search(r'([\\'?_:$!%^&*+\\\"”<>]|\\.{1,}|,{2,}).*?\\?$', x))))\n",
        "        good_special_words.append(int(bool(re.search(r'[а-яА-Яa-zA-Z]\\.|\\'[а-яА-Яa-zA-Z]|[a-zA-Z]+:\\s?[a-zA-Z]+|«.*»', x))))\n",
        "        not_enough_words.append(int(bool(re.search(r'([а-яА-Яa-zA-Z]+[\\s]?){6,}', x)))) \n",
        "        bad_comma.append(int(not bool(re.search(r'[а-яА-Яa-zA-Z]+\\, [а-яА-Яa-zA-Z]+', x)))*int(',' in x))\n",
        "        normal_x = normalize_text(x)\n",
        "        no_expert.append(int(bool(any(w in normal_x.split() for w in no_expert_words[:6])))) \n",
        "     \n",
        "    X['end_question'] = pd.Series(end_question)\n",
        "    X['fst_capital'] = pd.Series(fst_capital)\n",
        "    X['no_expert_eto'] = pd.Series(no_expert_eto)\n",
        "    X['special_signs'] = pd.Series(special_signs)\n",
        "    X['good_special_words'] = pd.Series(good_special_words)\n",
        "    X['not_enough_words'] = pd.Series(not_enough_words)\n",
        "    X['no_expert'] = pd.Series(no_expert)\n",
        "    X['bad_comma'] = pd.Series(bad_comma)\n",
        "    \n",
        "    X.drop(X[X['end_question'] == 0].index, inplace=True)\n",
        "    X.drop(X[X['fst_capital'] == 0].index, inplace=True)\n",
        "    X.drop(X[(X['special_signs'] == 1) & (X['good_special_words'] == 0)].index, inplace=True)\n",
        "    X.drop(X[X['bad_comma'] == 1].index, inplace=True)\n",
        "    X.drop(X[X['no_expert_eto'] == 1].index, inplace=True)\n",
        "    X.drop(X[X['not_enough_words'] == 0].index, inplace=True)\n",
        "    X.drop(X[X['no_expert'] == 1].index, inplace=True)\n",
        "    \n",
        "    #X['Question'] = list(map(normalize_text, X['Question']))\n",
        "    \n",
        "    print(X.shape)\n",
        "\n",
        "    return X[['ID', 'Question']]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2yptO7bDNjr"
      },
      "source": [
        "X_train = train.iloc[:, :2] \n",
        "y_train = train.iloc[:, 2]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDcsgfdmD6Zg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb6ef25-e99e-492f-cfe0-a09f246e2781"
      },
      "source": [
        "non_labeled_data = get_preprocess_data(X_train) # filter all simple sentences\n",
        "train = pd.merge(non_labeled_data, y, on='ID')\n",
        "train.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [00:01<00:00, 22643.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(21014, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21014, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ7zbG-HNRTT"
      },
      "source": [
        "# AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZPQ-EroiAlj"
      },
      "source": [
        "X, y = train[\"Question\"].values, train[\"Answer\"].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X, y = X_train, y_train\n",
        "X_train = np.array(X_train).reshape(-1, 1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kbQt3giiAtY"
      },
      "source": [
        "class QuestionsDataset(Dataset):\n",
        "    \"\"\" \n",
        "    This class make anchors, positives, negatives samples for TripletMarginLoss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: corpus of TF-IDF vectors\n",
        "            labels: labels for TF-IDF vectors\n",
        "        \"\"\"\n",
        "        self.questions = data\n",
        "        \n",
        "        self.ans = labels\n",
        "        self.positives = data[np.array(labels) == 1]\n",
        "        self.negatives = data[np.array(labels) == 0]\n",
        "        self.pos_idxs = np.array(range(labels.shape[0]))[labels == 1]\n",
        "        self.neg_idxs = np.array(range(labels.shape[0]))[labels == 0]\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.pos_key = True # this param controls dataset imbalance\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.ans.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # forget this idx we will make a new one\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        if self.pos_key == True:\n",
        "            self.pos_key = False\n",
        "            idx = random.choice(self.pos_idxs) # if self.pos_key == True, we take a expert question\n",
        "            batch_anchors = torch.from_numpy(self.questions[idx])\n",
        "            label = self.ans[idx] # for future experiments\n",
        "\n",
        "            batch_positives = torch.from_numpy(np.array(random.choice(self.positives))) \n",
        "            batch_negatives = torch.from_numpy(np.array(random.choice(self.negatives)))\n",
        "        else:\n",
        "            self.pos_key = True\n",
        "            idx = random.choice(self.neg_idxs) # if self.pos_key == False, we take a non-expert question\n",
        "            batch_anchors = torch.from_numpy(self.questions[idx])\n",
        "            label = self.ans[idx] # for future experiments\n",
        "\n",
        "            batch_positives = torch.from_numpy(np.array(random.choice(self.negatives)))\n",
        "            batch_negatives = torch.from_numpy(np.array(random.choice(self.positives)))\n",
        "\n",
        "        return batch_anchors, batch_positives, batch_negatives, label"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf0-6qq9k2-p"
      },
      "source": [
        "Next cell transform all text data into a TF-IDF vectors "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UharoFvumMWp"
      },
      "source": [
        "N, max_features must be tuned. N=10, max_features=1000 will get ~0.78 score after merging with filtered simple data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r738p8gfRBNa"
      },
      "source": [
        "N = 10 # ngram_range [1;N]\n",
        "max_features = 1000 \n",
        "transformer = TfidfVectorizer(ngram_range=(1, N), max_features=max_features, analyzer='char_wb')\n",
        "X_train1 = transformer.fit_transform(X_train).toarray()\n",
        "X_test1 = transformer.transform(X_test).toarray()\n",
        "X_val1 = transformer.transform(X_val).toarray()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7GYB7wPUFhj"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrKVb-SoiAvh"
      },
      "source": [
        "# make model deterministic\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "class AE(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.encoder_hidden_layer = nn.Linear(\n",
        "            in_features=kwargs[\"input_shape\"], out_features=1024\n",
        "        )\n",
        "        self.encoder_output_layer = nn.Linear(\n",
        "            in_features=1024, out_features=512\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(p=0.5, inplace=False)\n",
        "        \n",
        "        self.decoder_hidden_layer = nn.Linear(\n",
        "            in_features=512, out_features=1024\n",
        "        )\n",
        "        self.decoder_output_layer = nn.Linear(\n",
        "            in_features=1024, out_features=kwargs[\"input_shape\"]\n",
        "        )\n",
        "\n",
        "    def get_codes(self, features):\n",
        "        activation = self.encoder_hidden_layer(features)\n",
        "        activation = torch.relu(activation) \n",
        "        code = self.encoder_output_layer(activation)\n",
        "        \n",
        "        code = torch.relu(code)\n",
        "        return code\n",
        "\n",
        "    def forward(self, features):\n",
        "        activation = self.encoder_hidden_layer(features)\n",
        "        activation_1 = torch.relu(activation) # make new non-linear representation of TF-IDF vectors\n",
        "        activation = self.dropout(activation) # add noise to get stable training. Potentially can add resistance to small errors in words (But I don't think that this is good for question classification).  \n",
        "        \n",
        "        code = self.encoder_output_layer(activation)\n",
        "        code = torch.relu(code) # make code \n",
        "        \n",
        "        activation_2 = self.decoder_hidden_layer(code) # restore non-linear representation of TF-IDF vectors for optimization\n",
        "        return code, activation_1, activation_2\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = AE(input_shape=max_features).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2) #nn.MSELoss()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl8dDNwGUFhk"
      },
      "source": [
        "## Aplication on data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SbMQuW4MZq-"
      },
      "source": [
        "train_dataset = QuestionsDataset(X_train1, y_train)\n",
        "test_dataset = QuestionsDataset(X_test1, y_test)\n",
        "val_dataset = QuestionsDataset(X_val1, y_val)\n",
        "\n",
        "# make dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=1000, shuffle=True, num_workers=0, pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=1000, shuffle=True, num_workers=0, pin_memory=True\n",
        ")\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9n2ULSKrkEh"
      },
      "source": [
        "First stage: Pretrain AutoEncoder without tripletloss (Because I don't want cluster noise vectors) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd2w6KuBMdtz"
      },
      "source": [
        "for epoch in range(7):\n",
        "    loss = 0\n",
        "    for anchor, positive, negative, labels in train_loader:\n",
        "        \n",
        "        anchor, positive, negative = anchor.float().to(device), positive.float().to(device), negative.float().to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # compute reconstructions\n",
        "        anchor_out, rec_anc, rec_anc1 = model(anchor)\n",
        "        \n",
        "        positive_out, rec_pos, _ = model(positive)\n",
        "        negative_out, rec_neg, _ = model(negative)\n",
        "        \n",
        "        train_loss =  100 * criterion(rec_anc, rec_anc1)\n",
        "        \n",
        "        train_loss.backward()\n",
        " \n",
        "        optimizer.step()\n",
        "        \n",
        "        loss += train_loss.item()\n",
        "    \n",
        "    val_loss = 0\n",
        "    loss = loss / len(train_loader)\n",
        "    \n",
        "    for anchor, positive, negative, labels in val_loader:\n",
        "        anchor, positive, negative = anchor.float().to(device), positive.float().to(device), negative.float().to(device)\n",
        "\n",
        "        # compute reconstructions\n",
        "        anchor_out, rec_anc, rec_anc1 = model(anchor)\n",
        "\n",
        "        positive_out, rec_pos, _ = model(positive)\n",
        "        negative_out, rec_neg, _ = model(negative)\n",
        "\n",
        "        validation_loss = 100 * criterion(rec_anc, rec_anc1)\n",
        "        val_loss += validation_loss.item()\n",
        "\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    if abs(val_loss - train_loss) > 0.0005:\n",
        "        break\n",
        "    # display the epoch training loss\n",
        "    print(\"epoch : {}/{}, loss = {:.6f}, val_loss = {:.6f}\".format(epoch + 1, 7, loss, val_loss))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhhWWtkDsQtK"
      },
      "source": [
        "Second stage: Train AutoEncoder with TripletLoss to make expert/non-expert clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EFrWEj4p1rf",
        "outputId": "e104dbb6-e800-4b77-c63e-bf9f9ed7f720"
      },
      "source": [
        "for epoch in range(5):\n",
        "    loss = 0\n",
        "    for anchor, positive, negative, labels in train_loader: \n",
        "        anchor, positive, negative = anchor.float().to(device), positive.float().to(device), negative.float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # compute reconstructions\n",
        "        anchor_out, rec_anc, rec_anc1 = model(anchor)\n",
        "        positive_out, rec_pos, _ = model(positive)\n",
        "        negative_out, rec_neg, _ = model(negative)\n",
        "\n",
        "        # compute training reconstruction loss\n",
        "        # NB: first triplet for codes, second triplet is experimental (I assumed that if I do clusterization before codes it will improve codes clusterization )\n",
        "        train_loss =  100 * criterion(rec_anc, rec_anc1) + triplet_loss(anchor_out, positive_out, negative_out) + triplet_loss(rec_anc, rec_pos, rec_neg)\n",
        "        \n",
        "        # compute accumulated gradients\n",
        "        train_loss.backward()\n",
        "        \n",
        "        # perform parameter update based on current gradients\n",
        "        optimizer.step()\n",
        "        \n",
        "        # add the mini-batch training loss to epoch loss\n",
        "        loss += train_loss.item()\n",
        "    \n",
        "    # compute the epoch training loss\n",
        "    val_loss = 0\n",
        "    loss = loss / len(train_loader)\n",
        "    \n",
        "    for anchor, positive, negative, labels in val_loader:\n",
        "        anchor, positive, negative = anchor.float().to(device), positive.float().to(device), negative.float().to(device)\n",
        "\n",
        "        # compute reconstructions\n",
        "        anchor_out, rec_anc, rec_anc1 = model(anchor)\n",
        "        positive_out, rec_pos, _ = model(positive)\n",
        "        negative_out, rec_neg, _ = model(negative)\n",
        "\n",
        "        validation_loss = 100 * criterion(rec_anc, rec_anc1) + triplet_loss(anchor_out, positive_out, negative_out) + triplet_loss(rec_anc, rec_pos, rec_neg)\n",
        "        val_loss += validation_loss.item()\n",
        "    \n",
        "    val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    #if abs(val_loss - train_loss) > 0.06:\n",
        "    #  break\n",
        "    # display the epoch training loss\n",
        "    print(\"epoch : {}/{}, loss = {:.6f}, val_loss = {:.6f}\".format(epoch + 1, 5, loss, val_loss))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch : 1/5, loss = 2.004185, val_loss = 2.001757\n",
            "epoch : 2/5, loss = 1.999453, val_loss = 1.987203\n",
            "epoch : 3/5, loss = 1.973693, val_loss = 1.969549\n",
            "epoch : 4/5, loss = 1.946391, val_loss = 1.934149\n",
            "epoch : 5/5, loss = 1.927349, val_loss = 1.953159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1ACd6vGt7Jf"
      },
      "source": [
        "Apply simple LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVynPDDwNCyl",
        "outputId": "c6dd1dd6-089e-4b8f-87c3-dd61bafba797"
      },
      "source": [
        "# encode data with AutoEncoder for LogisticRegression\n",
        "\n",
        "X_train_enc = model.get_codes(torch.from_numpy(X_train1).to(device).float()).detach().cpu().numpy()\n",
        "X_val_enc = model.get_codes(torch.from_numpy(X_val1).to(device).float()).detach().cpu().numpy()\n",
        "X_test_enc = model.get_codes(torch.from_numpy(X_test1).to(device).float()).detach().cpu().numpy()\n",
        "\n",
        "print(X_train_enc.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13448, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9CuHoOTuIB3"
      },
      "source": [
        "Train logisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlL1l-03kVcR"
      },
      "source": [
        "clf1 = LogisticRegression(class_weight='balanced', random_state=1)\n",
        "\n",
        "logisticRegression = VotingClassifier(estimators=[('lr', clf1)], voting='soft')\n",
        "logisticRegression = logisticRegression.fit(X_train_enc, y_train)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nBaatpWu5HN"
      },
      "source": [
        "Test logisticRegression on hard samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtCBynh6luNh",
        "outputId": "a9f9cac8-f89c-4b58-c0a1-a1f7232e2299"
      },
      "source": [
        "y_pred = logisticRegression.predict_proba(X_train_enc)[:,1]\n",
        "\n",
        "y_true = y_train\n",
        "print(\"train\", roc_auc_score(y_true, y_pred), accuracy_score(y_true, logisticRegression.predict(X_train_enc)))\n",
        "\n",
        "y_pred = logisticRegression.predict_proba(X_test_enc)[:,1]\n",
        "y_true = y_test\n",
        "print(\"test\", roc_auc_score(y_true, y_pred), accuracy_score(y_true, logisticRegression.predict(X_test_enc)))\n",
        "\n",
        "y_pred = logisticRegression.predict_proba(X_val_enc)[:,1]\n",
        "y_true = y_val\n",
        "print(\"val\", roc_auc_score(y_true, y_pred), accuracy_score(y_true, logisticRegression.predict(X_val_enc)))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train 0.6934183586841189 0.6289411064842356\n",
            "test 0.6564822806036892 0.6276469188674756\n",
            "val 0.6537615823235923 0.6155218554861731\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFBi_ikV2nj2"
      },
      "source": [
        "from joblib import dump, load\n",
        "dump(logisticRegression, 'filename.joblib') # save logisticRegression\n",
        "torch.save(model.state_dict(), 'net') # save AutoEncoder"
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}